---
title: ""
date: ""
header-includes:
- \usepackage[utf8]{inputenc}
- \usepackage[spanish]{babel}
- \usepackage{graphicx}
- \usepackage{multirow,rotating}
- \pagenumbering{gobble}
- \usepackage{dcolumn}
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    includes:
      in_header: labels.tex
      before_body: cover.tex
csl: apa.csl
bibliography: fuentes1.bib
---

```{=tex}
\pagenumbering{gobble}
\pagenumbering{arabic}
```


```{r setup, include=FALSE }
knitr::opts_chunk$set(echo = T, fig.width = 6, fig.height = 3.5)

```

```{r, message=FALSE, include=FALSE,warning=FALSE, background=FALSE, comment=FALSE, engine.path=FALSE, cache=FALSE, out.extra=FALSE, results='hide'}
#rm(list = ls())
pacman::p_load(tidyverse,ISLR,forcats,broom,factoextra,gridExtra,psych,car,nortest,
               kableExtra,data.table,NbClust,cowplot,clValid,factorextra,leaps,
               stargazer,knitr,viridis,dplyr,readr,scales,quantmod,texreg,tinytex, 
               tidyr, imager,lubridate,tseries, astsa, growthrates, tis, dynlm, 
               readxl, foreign, hrbthemes, gtsummary, corrplot, lm.beta, ggfortify,
               AER, lmtest, sandwich,GGally, ggplot2, multcomp, purrr, VGAM, lessR,
               flextable, performance, see,qqplotr, ggrepel, patchwork,boot,rempsyc,
               report, ggResidpanel,DHARMa, SuppDists, glmnet)
```

\newpage 


# Ejercicio 1

Considere el modelo de regresión
$$
y= \beta_{0}+\beta_{1}x_{1}+...+\beta_{p}x_{p}+\epsilon
$$
y los estimadores obtenidos por mínimos cuadrados en forma matricial $\hat{\beta}=(X^tX)^{-1}X^ty$

Usando la matriz proyección $H$ y sus propiedades indique:

I) A que es igual: $e^tX$

$$
\begin{aligned} 
e^tX & = (y-\hat{y})^tX \\ & 
= (y- Hy)^tX \\ &
= (y^t-y^tH^t)X \\ &
= y^tX-y^tHX \\ &
= y^tX-y^t(X(X^tX)^{-1}X^t)X \\ &
= y^tX-y^tX[(X^tX)^{-1}X^tX] \\ &
= y^tX-y^tX = 0 
\end{aligned}
$$

$\therefore$ $e^tX=0$ 

II) A que es igual: $Cov(e,\hat{y})$

Primero notemos lo siguiente:
$$
\begin{aligned} 
e & = (y-\hat{y)} \\ & 
= (y-Hy) \\ &
= (I-H)y \\ &
\end{aligned}
$$
Donde $I$ es la matriz identidad, entonces tenemos:
$$
\begin{aligned} 
Cov(e,\hat{y}) & = Cov((I-H)y,\hat{y}) \\ & 
= Cov((I-H)y,Hy) \\ &
= (I-H)Cov(y,y)H^t \\ &
= (I-H)Var(y)H \\ &
= (I-H)\sigma^2 H \\ &
= \sigma^2(H-HH) \\ &
= \sigma^2(H-H) \\ &
= \sigma^2(0) = 0
\end{aligned}
$$
$\therefore$ $Cov(e,\hat{y})=0$



\newpage

# Ejercicio 2

Considere el modelo de regresion
$$
y_i=\beta_0+\beta_1 x_i+\beta_2\left(3 x_i^2-2\right)+\xi_i \quad i=1,2,3
$$
donde
$$
x_1=-1, x_2=0, x_3=1
$$
I)
Matriz de diseño
$$
X=\left(\begin{array}{rrr}
1 & -1 & 1 \\
1 & 0 & -2 \\
1 & 1 & 1
\end{array}\right) \text { pues } \begin{aligned}
& 3 x_1^2-2=1 \\
& 3 x_2^2-2=-2 \\
& 3 x_3^2-2=1
\end{aligned}
$$

Asi
$$
\begin{aligned}
& X^t X=\left(\begin{array}{ccc}
1 & 1 & 1 \\
-1 & 0 & 1 \\
1 & -2 & 1
\end{array}\right)\left(\begin{array}{ccc}
1 & -1 & 1 \\
1 & 0 & -2 \\
1 & 1 & 1
\end{array}\right)=\left(\begin{array}{lll}
3 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 6
\end{array}\right) \\
& y \\
& \left(X^t X\right)^{-1}=\left(\begin{array}{ccc}
1 / 3 & 0 & 0 \\
0 & 1 / 2 & 0 \\
0 & 0 & 1 / 6
\end{array}\right)
\end{aligned}
$$
II)

$$
\hat{\beta}=(X^t X)^{-1} X^t y
$$
asi:
$$
\begin{aligned}
& \hat{\beta}=\left(\begin{array}{ccc}
1 / 3 & 0 & 0 \\
0 & 1 / 2 & 0 \\
0 & 0 & 1 / 6
\end{array}\right)\left(\begin{array}{ccc}
1 & 1 & 1 \\
-1 & 0 & 1 \\
1 & -2 & 1
\end{array}\right)\left(\begin{array}{l}
y_1 \\
y_2 \\
y_3
\end{array}\right)=\left(\begin{array}{ccc}
1 / 3 & 1 / 3 & 1 / 3 \\
-1 / 2 & 0 & 1 / 2 \\
1 / 6 & -1 / 3 & 1 / 6
\end{array}\right)\left(\begin{array}{l}
y_1 \\
y_2 \\
y_3
\end{array}\right) \\
& \hat{\beta}=\left(\begin{array}{l}
\frac{1}{3}\left(y_1+y_2+y_3\right) \\
\frac{1}{2}\left(y_3-y_1\right) \\
\frac{1}{6}\left(y_1-2 y_2+y_3\right)
\end{array}\right)
\end{aligned}
$$

Por lo tanto
$$
\hat{\beta}_0=\frac{1}{3}\left(y_1+y_2+y_3\right) \quad \hat{\beta}_1=\frac{1}{2}\left(y_3-y_1\right) \quad \hat{\beta}_2=\frac{1}{6}\left(y_1-2 y_2+y_3\right)
$$
III)
Obtenemos los estimadores del modelo reducido:
$$
y_i=\beta_0^*+\beta_1^* x_i+\xi_i^* \quad i=1,2,3
$$

Obtenemos
$$
\begin{aligned}
& \left(X^t X\right)^{-1}=\left(\begin{array}{cc}
1 / 3 & 0 \\
0 & 1 / 2
\end{array}\right) \\
&
\end{aligned}
$$
$$\begin{aligned} & \rightarrow \hat{\beta}^*=\left(\begin{array}{cc}1 / 3 & 0 \\ 0 & 1 / 2\end{array}\right)\left(\begin{array}{ccc}1 & 1 & 1 \\ -1 & 0 & 1\end{array}\right)\left(\begin{array}{l}y_1 \\ y_2 \\ y_3\end{array}\right)=\left(\begin{array}{ccc}1 / 3 & 1 / 3 & 1 / 3 \\ -1 / 2 & 0 & 1 / 2\end{array}\right)\left(\begin{array}{l}y_1 \\ y_2 \\ y_3\end{array}\right) \\ & \hat{\beta}^*=\binom{\frac{1}{3}\left(y_1+y_2+y_3\right)}{\frac{1}{2}\left(y_3-y_1\right)} \\ & \text { Asi } \hat{\beta}_0^*=\frac{1}{3}\left(y_1+y_2+y_3\right) \text { y } \hat{\beta}_1^*=\frac{1}{2}\left(y_3-y_1\right) \\ \therefore \hat{\beta}_0^*=\hat{\beta}_0 \text { y } \hat{\beta}_1^*=\hat{\beta}_1 \\ & \end{aligned}$$


\newpage

# Ejercicio 3

Compararemos 4 distintos diseños de empaque de un nuevo cereal, asignados aleatoriamente a 5 tiendas como unidades muestrales, y las ventas en un periodo de 2 semanas. 

```{r, echo=FALSE}
ventas<-c(12,10,15,17,11,11,17,16,14,15,27,34,22,26,28,23,20,18,17)
cereal<-c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4)
data<-cbind(ventas,cereal)
data<-as.data.frame(data)
data$cereal<-as.factor(data$cereal)
```

En el siguiente Boxplot, podemos observar que el empaque que más se vendió es el 3, aunque con una mayor variabilidad entre las tiendas que las venden y el empaque que menos se vendió es el empaque 1. 


```{r, echo=FALSE, fig.height=2.8, fig.width=6}
data %>%
  ggplot( aes(x=cereal, y=ventas, fill=cereal)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6, option="A") + theme_bw()+
    theme(
      legend.position="none",
      plot.title = element_text(size=11)
    ) +
    ggtitle("Ventas por tipo de cereal") +
    xlab("")
```

Ajustaremos un modelo de regresión lineal múltiple del número de ventas promedio por cada tipo de empaque. 

```{r, echo=FALSE}
modelo<-lm(data=data, ventas~cereal)
summary(modelo)
```

Las expresiones del número de ventas promedio por cada tipo de empaque son.

```{r, echo=FALSE}
b0<-modelo$coefficients[1]
b1<-modelo$coefficients[2]
b2<-modelo$coefficients[3]
b3<-modelo$coefficients[4]
b0_b1<-modelo$coefficients[1]+modelo$coefficients[2]
b0_b2<-modelo$coefficients[1]+modelo$coefficients[3]
b0_b3<-modelo$coefficients[1]+modelo$coefficients[4]
```



$E(ventas; cereal1)=\hat{\beta_0}=$ `r b0` 

$E(ventas; cereal2)=\hat{\beta_0} + \hat{\beta_1}=$ `r b0` + `r b1` = `r b0_b1`

$E(ventas; cereal3)=\hat{\beta_0} + \hat{\beta_2}=$ `r b0` + `r b2` = `r b0_b2`

$E(ventas; cereal4)=\hat{\beta_0} + \hat{\beta_3}=$ `r b0` + `r b3` = `r b0_b3`


Las hipótesis que se contrastan con la prueba F asociada a la tabla ANOVA son, la hipotesis nula $H_0: \hat{\beta_1}=\hat{\beta_2}=\hat{\beta_3}=0$ contra la alternativa de $H_a: \hat{\beta_i} \neq0$ para al menos un $i=1,2,3$. Esta prueba se presenta en la salida o ``summary`` anterior, en donde podemos observar un $p-value: 1.367e-05$ de la prueba ``F-statistic`` con valor de $20.71$ con $3$ y $15$ grados de libertad. Como el valor del $p-value <0.05$, i.e., considerando un nivel de significancia estadística $\alpha=0.05$, podemos concluir que se rechaza la hipótesis nula $H_0$, por lo que al menos un $\beta_i$ es distinto de cero en el modelo planteado. 


Para ver si el diseño del empaque afecta las ventas promedio, plantearemos algunas pruebas de hipótesis, usando un nivel de confianza del 95%. Nos preguntamos si $E(ventas; cereal1) \neq E(ventas; cereal2)$,  $E(ventas; cereal1) \neq E(ventas; cereal3)$, $E(ventas; cereal1) \neq E(ventas; cereal4)$, $E(ventas; cereal2) \neq E(ventas; cereal3)$, $E(ventas; cereal2) \neq E(ventas; cereal4)$, $E(ventas; cereal3) \neq E(ventas; cereal4)$. Entonces, planteamos las siguientes pruebas.

Planteamiento de la hipótesis nula:

$\hat{\beta_0}=\hat{\beta_0} + \hat{\beta_1} \rightarrow \hat{\beta_1} =0$

$\hat{\beta_0}=\hat{\beta_0} + \hat{\beta_2} \rightarrow \hat{\beta_2} =0$

$\hat{\beta_0}=\hat{\beta_0} + \hat{\beta_3} \rightarrow \hat{\beta_3} =0$

$\hat{\beta_0} + \hat{\beta_1}=\hat{\beta_0} + \hat{\beta_2} \rightarrow \hat{\beta_1} =\hat{\beta_2}$

$\hat{\beta_0} + \hat{\beta_1}= \hat{\beta_0} + \hat{\beta_3}  \rightarrow \hat{\beta_1} =\hat{\beta_3}$

$\hat{\beta_0} + \hat{\beta_2}= \hat{\beta_0} + \hat{\beta_3}  \rightarrow \hat{\beta_2} =\hat{\beta_3}$

Tenemos términos redundantes, por lo que nos quedaría la siguiente prueba de hipótesis. 

$H_0: \hat{\beta_1}=\hat{\beta_2}=\hat{\beta_3}=0$ VS $H_a: \hat{\beta_i}\neq0$ p.a. $i=1,2,3$. 

En la prueba global $F$ asociada a la tabla ANOVA descrita anteriormente, se rechzó $H_0$. Por lo que podemos concluir que al menos un diseño de empaque afecta las ventas promedio, sin embargo no nos dice explícitamente cuál o cuáles en un análisis simultáneo.   

Para esto, podemos plantear una prueba de hipótesis simultánea asociada a la igualdad de las ventas promedio entre todos los posibles pares de diferentes empaques, que puede resolverse con la prueba lineal general simultánea. A continuación, se muestra la salida de la prueba. 

```{r, echo=FALSE}
K=matrix(c(0,1,0,0,
           0,0,1,0,
           0,0,0,1,
           0,1,-1,0,
           0,1,0,-1,
           0,0,1,-1), ncol=4, nrow=6, byrow=TRUE)
m=c(0,0,0,0,0,0)
summary(glht(modelo, linfct=K, rhs=m))
```

Finalmente realizaremos una prueba de hipótesis para argumentar en favor o en contra de la hipótesis de que el diseño de empaque 3 es el que más aumenta las ventas en comparación con el resto de empaques. Esto es, que $E(ventas; cereal3)>E(ventas; cereal1)$, $E(ventas; cereal3)>E(ventas; cereal2)$, $E(ventas; cereal3)>E(ventas; cereal4)$. Entonces planteamos, las siguientes hipótesis.   

$\hat{\beta_0} + \hat{\beta_2}> \hat{\beta_0} \rightarrow \hat{\beta_2}>0$

$\hat{\beta_0} + \hat{\beta_2}>\hat{\beta_0} + \hat{\beta_1} \rightarrow \hat{\beta_2}>\hat{\beta_1}$

$\hat{\beta_0} + \hat{\beta_2}>\hat{\beta_0} + \hat{\beta_3} \rightarrow \hat{\beta_2}>\hat{\beta_3}$


Hipótesis nula: $H_0: \hat{\beta_2}\leq 0$, $\hat{\beta_2}\leq \hat{\beta_1}$, $\hat{\beta_2}\leq \hat{\beta_3}$

Hipótesis alternativa: $H_a: \hat{\beta_2}> 0$, $\hat{\beta_2} > \hat{\beta_1}$, $\hat{\beta_2} >\hat{\beta_3}$

A continuación se muestra la salida, donde podemos observar que se rechaza $H_0$, por lo que podemos afirmar que el diseño de empaque 3 es el que más aumenta las ventas en comparación con el resto de empaques.  

```{r, echo=FALSE}
K=matrix(c(0,0,1,0,
           0,-1,1,0,
           0,0,1,-1), ncol=4, nrow=3, byrow=TRUE)
m=c(0,0,0)
summary(glht(modelo, linfct=K, rhs=m, alternative="great"))
```


\newpage

# Ejercicio 4

```{r, include=FALSE}
library(multcomp)
datos <- read.csv("Ex4B.csv")
str(datos)
datos$Sexo=factor(datos$Sexo)
datos$Trat=factor(datos$Trat)
str(datos)
levels(datos$Sexo)
levels(datos$Trat)
```

I) 

```{r, echo=FALSE, fig.height=3.5, fig.width=8}
par(cex.axis=0.8) 
boxplot(Puntaje ~ Trat+Sexo, data = datos, col = "white", outline=FALSE)
```

En estos boxplots podemos observar que para el caso Control se observa que de puntaje
que presentan ansiedad, tamto en hombres como mujeres, se encuentra aproximadamente entre 10 y 11.

En el caso de aplicar el tratamiento actual (Trat1) podemos notar que hay una disminucion
en el nivel de ansiedad que presentan tanto hombres como mujeres, aunque parece ser que el 
tratamiento actual reduce más el nivel de ansiedad en mujeres que en hombres.

Por otro lado, al aplicar el nuevo tratamiento (Trat2) podemos notar una ligera disminucion
en el nivel de ansiedad de los hombres respecto al caso Control, pero los niveles de ansiedad
son mayores en comparacion al aplicar el tratamiento actual. Mientras que en el caso de las
mujeres el nuevo tratamiento reduce la ansiedad a un nivel muchisimo mas bajo en comparación
al caso control e incluso a un nivel mas bajo comparandolo al aplicar el tratamiento actual.


II) 


El modelo general es el siguiente:
$$
E(Puntaje;Trat,Sexo) = \beta_{0} + \beta_{1}*Trat1 + \beta_{2}*Trat2 + \beta_{3}*Mujer + \beta_{4}(Trat1*Mujer) + \beta_{5}(Trat2*Mujer)
$$
A partir del modelo general podemos obtener los modelos individuales:

$E(Puntaje;Control,Hombre) = \beta_{0}$

$E(Puntaje;Trat1,Hombre) = \beta_{0} + \beta_{1}$

$E(Puntaje;Trat2,Hombre) = \beta_{0} + \beta_{2}$

$E(Puntaje;Control,Mujer) = \beta_{0} + \beta_{3}$

$E(Puntaje;Trat1,Mujer) = \beta_{0} + \beta_{1} + \beta_{3} + \beta_{4}$ 

$E(Puntaje;Trat2,Mujer) = \beta_{0} + \beta_{2} + \beta_{3} + \beta_{5}$

Ajunstamos nuestro modelo:

```{r, echo=FALSE}
fit <- lm(Puntaje ~ Trat*Sexo, data = datos)
summary(fit)
```

Por tanto las estimaciones puntuales son:

$E(Puntaje;Control,Hombre) = 10.7602$

$E(Puntaje;Trat1,Hombre) = 10.7602 + (-1.51) = 9.2502$

$E(Puntaje;Trat2,Hombre) = 10.7602 + (-0.4798) = 10.2804$

$E(Puntaje;Control,Mujer) = 10.7602 + 0.5231 = 11.2833$

$E(Puntaje;Trat1,Mujer) = 10.7602 + (-1.5100) + 0.5231 + (-1.3758) = 8.3975$ 

$E(Puntaje;Trat2,Mujer) = 10.7602 + (-0.4789) + 0.5231 + (-3.5914) = 7.213$


III) 


Las hipótesis que se contrastan con la tabla ANOVA son:

$H_0 : \beta_{0}=0$, $\beta_{1}=0$, $\beta_{2}=0$, $\beta_{3}=0$, $\beta_{4}=0$, $\beta_{5}=0$  vs  $H_{\alpha} : \beta_{0}\neq0$ ó $\beta_{1}\neq0$ $\beta_{2}\neq0$, $\beta_{3}\neq0$, $\beta_{4}\neq0$, $\beta_{5}\neq0$

Con el comando $summary(fit)$ vemos que el p-value asociado a la tabla ANOVA es de 1.873e-11 que es menor a nuestra significancia de .05, por lo que rechazamos $H_{0}$.

IV)

Para determinar si el sexo tiene un efecto en el puntaje realizaremos una prueba de hipotesis, la cual tiene como hipotesis nula:
$$
H_{0} = \left\{ \begin{array}{rcl}
E(Puntaje;Control,Hombre) & \mbox{=}
& E(Puntaje;Control,Mujer) \\  E(Puntaje;Trat1,Hombre) & \mbox{=}
& E(Puntaje;Trat1,Mujer) \\ 
E(Puntaje;Trat2,Hombre) & \mbox{=}
& E(Puntaje;Trat2,Mujer)
\end{array}\right. \Longleftrightarrow
$$

$$
\Longleftrightarrow H_{0} = \left\{ \begin{array}{rcl}
\beta_{0} & \mbox{=} & \beta_{0} + \beta_{3} \\  
\beta_{0} + \beta_{1} & \mbox{=} & \beta_{0} + \beta_{1} + \beta_{3} + \beta_{4} \\ 
\beta_{0} + \beta_{2} & \mbox{=} & \beta_{0} + \beta_{2} + \beta_{3} + \beta_{5}
\end{array}\right.\Longleftrightarrow
H_{0} = \left\{ \begin{array}{rcl}
0 & \mbox{=} & \beta_{3} \\  
0 & \mbox{=} & \beta_{3} + \beta_{4} \\ 
0 & \mbox{=} & \beta_{3} + \beta_{5}
\end{array}\right.
$$

Notemos que al comparar dos a dos las igualdades de $H_{0}$ podemos obtener una hipotesis nula que es equivalente, la cual es:
$$
H_{0} = \left\{ \begin{array}{rcl}
0 & \mbox{=} & \beta_{3} \\  
0 & \mbox{=} & \beta_{4} \\ 
0 & \mbox{=} & \beta_{5}
\end{array}\right.
$$
Por lo tanto, la prueba de hipotesis a realizar es:

$H_0 : \beta_{3}=0$, $\beta_{4}=0$, $\beta_{5}=0$  vs  $H_{\alpha}: \beta_{3}\neq0$, $\beta_{4}\neq0$, $\beta_{5}\neq0$

```{r, echo=FALSE}
K=matrix(c(0,0,0,1,0,0,
            0,0,0,0,1,0,
            0,0,0,0,0,1), ncol=6, nrow=3, byrow=TRUE)
m=c(0,0,0)
summary(glht(fit, linfct=K, rhs=m), test=Ftest())
```
Vemos que el p-valor es de 1.827792e-06, por lo que rechazamos $H_{0}$. Ahora realizaremos una prueba simultanea para ver si podemos decir que el sexo tiene algun efecto en el tratamiento 

```{r, echo=FALSE}
summary(glht(fit, linfct=K, rhs=m))
```
Con la prueba simultanea podemos ver que es plausible quitar a $\beta_{3}$ y $\beta_{4}$ del modelo, pues el p-value en ambos es mayor a .025

Por tanto nuestro modelo reducido seria:
$$
E(Puntaje;Trat,Sexo) = \beta_{0} + \beta_{1}*Trat1 + \beta_{2}*Trat2 + \beta_{3}(Trat2*Mujer)
$$
V)

Ajustamos nuestro modelo reducido:
```{r, echo=FALSE}
fit2 <- lm(Puntaje ~ I(Trat=="Trat1")+I(Trat=="Trat2")+I((Sexo=="Mujer")*(Trat=="Trat2")), data = datos)
summary(fit2)
```
Entonces las expresiones del puntaje promedio de cada uno de los valores en las variables categoricas, junton con su estimacion puntual, son:

$E(Puntaje;Control,Hombre) = \beta_{0} = 11.0217$

$E(Puntaje;Trat1,Hombre) = \beta_{0} + \beta_{1} = 11.0217 + (-2.1979) = 8.8238$

$E(Puntaje;Trat2,Hombre) = \beta_{0} + \beta_{2} = 11.0217 + (-0.7405) = 10.2812$

$E(Puntaje;Control,Mujer) = \beta_{0} = 11.0217$

$E(Puntaje;Trat1,Mujer) = \beta_{0} + \beta_{1} = 11.0217 + (-2.1979) = 8.8238$ 

$E(Puntaje;Trat2,Mujer) = \beta_{0} + \beta_{2} + \beta_{3} = 11.0217 + (-0.7405) + (-3.0683) = 7.2129$

VI)

Queremos realizar una prueba de hipotesis para ver si el nuevo tratamiento tiene mejor desempeño, por lo que nuestra hipotesis alternativa seria:
$$
H_{\alpha} = \left\{ \begin{array}{rcl}
E(puntaje;Trat2,Hombre) & < & E(puntaje;Control,Hombre) \\  
E(puntaje;Trat2,Hombre) & < & E(puntaje;Trat1,Hombre) \\ 
E(puntaje;Trat2,Mujer) & < & E(puntaje;Control,Mujer) \\
E(puntaje;Trat2,Mujer) & < & E(puntaje;Trat1,Mujer)
\end{array}\right. \Longleftrightarrow 
$$

$$
\Longleftrightarrow  H_{\alpha} = \left\{ \begin{array}{rcl}
\beta_{0} + \beta_{2} & < & \beta_{0} \\  
\beta_{0} + \beta_{2} & < & \beta_{0} + \beta_{1} \\ 
\beta_{0} + \beta_{2} + \beta_{3} & < & \beta_{0} \\
\beta_{0} + \beta_{2} + \beta_{3} & < & \beta_{0} + \beta_{1}
\end{array}\right.\Longleftrightarrow 
H_{\alpha} = \left\{ \begin{array}{rcl}
0 & <& - \beta_{2} \\  
0 & < & \beta_{1} - \beta_{2} \\ 
0 & < & - \beta_{2} - \beta_{3} \\
0 & < & \beta_{1} - \beta_{2} - \beta_{3}
\end{array}\right.
$$


Entonces nuestra prueba de hipotesis a realizar es:
$$
H_{0} = \left\{ \begin{array}{rcl}
0 & \geq & - \beta_{2} \\  
0 & \geq & \beta_{1} - \beta_{2} \\ 
0 & \geq & - \beta_{2} - \beta_{3} \\
0 & \geq & \beta_{1} - \beta_{2} - \beta_{3}
\end{array}\right.
\mbox{vs} \quad
H_{\alpha} = \left\{ \begin{array}{rcl}
0 & < & - \beta_{2} \\  
0 & < & \beta_{1} - \beta_{2} \\ 
0 & <& - \beta_{2} - \beta_{3} \\
0 & < & \beta_{1} - \beta_{2} - \beta_{3}
\end{array}\right.
$$

```{r, echo=FALSE}
K2=matrix(c(0,0,-1,0,
            0,1,-1,0,
            0,-1,-1,0,
            0,1,-1,-1), ncol=4, nrow=4, byrow=TRUE)
m2=c(0,0,0,0)
summary(glht(fit2, linfct=K2, rhs=m2, alternative="greater"))
```
Con esta prueba podemos ver que hay evidencia para no rechazar H0, pues tenemos que en los hombres el nuevo tratamiento no resulta ser mejor. Por lo que podriamos decir que el nuevo tratamiento no tiene mejor desempeño

VII)

Queremos realizar una prueba de hipotesis para ver que el tratamiento nuevo tiene mejor desempeño en mujeres mientras que el tratamiento actual lo tiene en hombres, por lo que nuestra hipotesis alternativa seria:
$$
H_{\alpha} = \left\{ \begin{array}{rcl}
E(puntaje;Trat1,Hombre) & < & E(puntaje;Control,Hombre) \\  
E(puntaje;Trat1,Hombre) & < & E(puntaje;Trat2,Hombre) \\ 
E(puntaje;Trat2,Mujer) & < & E(puntaje;Control,Mujer) \\
E(puntaje;Trat2,Mujer) & < & E(puntaje;Trat1,Mujer)
\end{array}\right.
$$
$$
\Longleftrightarrow \\
$$
$$
H_{\alpha} = \left\{ \begin{array}{rcl}
\beta_{0} + \beta_{1} & < & \beta_{0} \\  
\beta_{0} + \beta_{1} & < & \beta_{0} + \beta_{2} \\ 
\beta_{0} + \beta_{2} + \beta_{3} & < & \beta_{0} \\
\beta_{0} + \beta_{2} + \beta_{3} & < & \beta_{0} + \beta_{1}
\end{array}\right.\Longleftrightarrow
H_{\alpha} = \left\{ \begin{array}{rcl}
0 & < & - \beta_{1} \\  
0 & < & \beta_{2} - \beta_{1} \\ 
0 & < & - \beta_{2} - \beta_{3} \\
0 & < & \beta_{1} - \beta_{2} - \beta_{3}
\end{array}\right.
$$

Entonces nuestra prueba de hipotesis a realizar es:
$$
H_{0} = \left\{ \begin{array}{rcl}
0 & \geq & - \beta_{1} \\  
0 & \geq & \beta_{2} - \beta_{1} \\ 
0 & \geq & - \beta_{2} - \beta_{3} \\
0 & \geq & \beta_{1} - \beta_{2} - \beta_{3}
\end{array}\right.
\mbox{vs} \quad
H_{\alpha} = \left\{ \begin{array}{rcl}
0 & < & - \beta_{1} \\  
0 & < & \beta_{2} - \beta_{1} \\ 
0 & < & - \beta_{2} - \beta_{3} \\
0 & < & \beta_{1} - \beta_{2} - \beta_{3}
\end{array}\right.
$$
```{r, echo=FALSE}
K3=matrix(c(0,-1,0,0,
            0,-1,1,0,
            0,0,-1,-1,
            0,1,-1,-1), ncol=4, nrow=4, byrow=TRUE)
m3=c(0,0,0,0)
summary(glht(fit2, linfct=K3, rhs=m3, alternative="greater"))
```
Con esta prueba podemos ver que todos los p-value son menores a .05, por lo que podemos decir que el medicamento actual es mejor en los hombres mientras que el nuevo medicamento tiene mejor desempeño en las mujeres.


\newpage

# Ejercicio 5

I)
```{r,include=FALSE}
Datos<-read.csv("Ex5.csv")
Datos$Trat=factor(Datos$Trat)
```

```{r, echo=FALSE}

plot(Datos$Edad,Datos$Ant,pch=19,col=c("blue","green")[Datos$Trat],xlab = "Edad",ylab = "Anticuerpos")
legend("bottomleft", levels(Datos$Trat),
       col = c("blue","green"), pch = 19, inset = 0.01,  pt.cex=1.5,cex = .9, y.intersp = 1.3 , bty="n")
```

Observamos un aumento en los anticuerpos de la poblacion con medicamento apartir de cierta edad, lo cual podria significar una diferencia en la pendiente con respecto a la poblacion control

II)

Tenemos el modelo con interacciones:
$$
E(y;x)= \beta_0 + \beta_1 + \beta_2 TratMed + \beta_3 (Edad*TratMed)
$$
```{r, echo=FALSE}
fit <- lm(Ant ~ Edad * Trat, data = Datos) 
summary(fit)
```
Se rechaza la hipotesis de que todas las $\beta$´s sean cero.

III)

a) $E(y; Trat:Contol;Edad)= \beta_0 + \beta_1 Edad$

   $E(y; Trat:Contol;Edad)= 29.34298 + (-0.2829)Edad$

b) $E(y; Trat:Med ;Edad)= (\beta_0+\beta_2) + (\beta_1+\beta_3) Edad$ 
  
   $E(y; Trat:Med ;Edad) = (29.34298-2.2573) + (-0.2829+0.17307)Edad$

IV)

Para corroborar si la edad afecta por igual a ambos grupos, se requiere la siguiente pueba de hipotesis:

 $H_0:\beta_3=0$ vs $H_a:\beta_3 \neq 0$

es decir se busca una diferencia en las pendientes
```{r,include=FALSE}
library(multcomp)
```

```{r, echo=FALSE}
K=matrix(c(0,0,0,1), ncol=4, nrow=1, byrow=TRUE)
m=c(0)
summary(glht(fit, linfct=K, rhs=m), test=Ftest())
```
De esta forma se rechaza $H_0$, por lo que se puede decir que la edad no afecta de la misma forma
al grupo control y al grupo que se aplico el medicamento.

V)

Este modelo parece indicar que el medicamento aumenta la produccion de anticuerpos pero dicho tratamiento es
mas efectivo en edades avanzadas.

$\beta_0$ se podria interpreta como una aproximacion de anticuerpos que la personas cercanas a los 20 años tienen sin que se les haya aplicado el tratamiento, de esta forma al ser $\beta_2$ pequeña nos dice que hay poca diferencia con
el numero de anticuerpos de a quienes si se les aplico y que igualmente tienen edad cercana a 20 años.

$\beta_1$ mide el como la edad afecta el numero de anticuerpos en el grupo sin tratamiento, al se $\beta_1$ negativa se podria decir que afecta negativamente, al ser $\beta_3$ positiva  el tratamiento reduce 
los efectos negativos de la edad respecto al numero de anticuerpos.

VI)

Bajamos la confianza al 90%

```{r,include=FALSE}
edades.interes<-seq(from=25,to=60,by=1)
length(edades.interes)


KC<-cbind(1,edades.interes,0,0)
KC

KT<-cbind(1,edades.interes,1,edades.interes)
KT
K=rbind(KC,KT)

fitE<-glht(fit,linfct = K)
fitci<-confint(fitE,level = 0.90)

```

```{r, echo=FALSE}
plot(Datos$Edad,Datos$Ant,pch=19,col=c("blue","green")[Datos$Trat],xlab = "Edad",ylab = "Anticuerpos")
legend("bottomleft", levels(Datos$Trat),
       col = c("blue","green"), pch = 19, inset = 0.01,  pt.cex=1.5,cex = .9, y.intersp = 1.3 , bty="n")

lines(edades.interes,coef(fitE)[1:36],col="red")
lines(edades.interes,fitci$confint[1:36,"upr"],col="red")
lines(edades.interes,fitci$confint[1:36,"lwr"],col="red")

lines(edades.interes,coef(fitE)[37:72],col="black")
lines(edades.interes,fitci$confint[37:72,"upr"],col="black")
lines(edades.interes,fitci$confint[37:72,"lwr"],col="black")
```

Observamos los intervalos de confianza no se intersectan en el rango de edades de 25 a 60
por lo que podemos decir que en estas edades el medicamento funciona.

\newpage

# Ejercicio 6

```{r, echo=FALSE}
datos6<-read.csv("Ex6.csv")
datos6$X1<-as.factor(datos6$X1)
datos6$X2<-as.factor(datos6$X2)
```

Con datos de ``Ex6.csv`` se considera un modelo de regresión lineal con las covariables $X_1$ a $X_6$ sin interacción. Se muestra el resultado en la primera columna del Cuadro de ``MODELOS``, en donde se observa la prueba global $F$ asociado a la tabla ANOVA cuyo p-value es menor a $0.05$, por lo que se rechaza la hipótesis nula $H_0: \hat{\beta_i}=0, \forall i=1,2,...,p$, a favor de la alternativa de que al menos un coeficiente estimado $\hat{\beta_i}$ es distinto de cero.  Se observa además que dado que están las otras variables, la variable X3 no agrega información adicional al modelado, lo mismo para el caso de X5. (Chunk modelo1, línea de código 34). 

```{r modelo1, echo=FALSE, include=FALSE}
modelo1<-lm(data=datos6, Y~.)
summary(modelo1)
```


```{r, echo=FALSE}
#stargazer(modelo1, no.space=TRUE)
```

Si hacemos una prueba visual de los supuestos del modelo, tales como la linealidad (Residuals vs Fitted),  homocedasticidad (Scale-Location), normalidad (Q-Q Residuals) y presencia de outliers influyentes (Residuals vs Leverage), se observa que no se cumple la linealidad y normalidad, aunque parece no haber problemas con la homocedasticidad y la presencia de outliers influyentes (que se salgan de la distancia de Cook). (Chunk plotsmodelo1, línea de código 47) 


```{r plotsmodelo1, echo=FALSE, fig.height=2.5, fig.width=6, include=FALSE}
par(mfrow=c(1,2))
#par(mar=c(4, 5, 3, 1))
plot(modelo1, 1)   #linealidad
plot(modelo1, 3)   #homocedasticidad
plot(modelo1, 2)   #normalidad
plot(modelo1, 5)   #Outliers 
```

```{r pruebasmodelo1, echo=FALSE, include=FALSE}
#Varianza constante 
#Se basa en los errores estandarizados o estudentizados
#Mismas pruebas usadas en regresión lineal simple:
library(lmtest)
sbpt1<-lmtest::bptest(modelo1) #NO se rechaza H0 de homocedasticidad, NO hay problemas
sbpt1p<-sbpt1$p.value #p-value de la prueba Breusch Pagan estudentizado

#Normalidad 
#Se basa en los residuales estandarizados o estudentizados
#Mismas pruebas que se usaron en regresi?n lineal simple:
library(broom)
Datosmodelo1=augment(modelo1)
swt1<-shapiro.test(Datosmodelo1$.std.resid) #Se rechaza H0 de normalidad, hay problemas
swt1p<-swt1$p.value
library(nortest)
kst1<-nortest::lillie.test(Datosmodelo1$.std.resid)#Se rechaza H0 de normalidad, hay problemas
kst1p<-kst1$p.value
library(tseries)
jbt1<-tseries::jarque.bera.test(Datosmodelo1$.std.resid)#Se rechaza H0 de normalidad, hay problemas
jbt1p<-jbt1$p.value
```

De acuerdo con la prueba ``studentized Breusch-Pagan`` se tiene un p-value de `r sbpt1p` por lo que no se rechaza la hipótesis nula de homocesaticidad, mientras que las pruebas de normalidad Jarque-Bera, Shapiro-Wilk y Kolmogorov-Smirnov rechazan la hipótesis nula de normalidad, con p-value de `r jbt1p`, `r swt1p` y `r kst1p`, respectivamente. (Chunk pruebasmodelo1, línea de código 56). 


```{r residualplotsmodelo1, echo=FALSE, fig.height=4, fig.width=7 , include=FALSE}
#par(mfrow=c(1,1))
library(car)
residualPlots_modelo1<-residualPlots(modelo1) #se observan problemas con X4 y X6
```

Para el caso de la linealidad, la prueba de Tukey rechaza la hipótesis nula de linealidad, particularmente con X4 y X6, el p-value asociado es menor a 0.05. (Chunk residualplotsmodelo1, línea de código 82). 

```{r, echo=FALSE}
residualPlots_modelo1[,2]
```

Esto se refleja en las gráficas individuales, para X4 y X6 es evidente la no linealidad. (Chunk residualPlotsmodelo1, línea de código 96). 

```{r residualPlotsmodelo1, echo=FALSE, warning=FALSE, fig.height=3, fig.width=7, results='hide',fig.keep='all', include=FALSE}
par(mfrow=c(1,2))
residualPlots(modelo1, terms= ~ X1,fitted=FALSE)
residualPlots(modelo1, terms= ~ X2,fitted=FALSE)
residualPlots(modelo1, terms= ~ X3,fitted=FALSE)
residualPlots(modelo1, terms= ~ X4,fitted=FALSE)
residualPlots(modelo1, terms= ~ X5,fitted=FALSE)
residualPlots(modelo1, terms= ~ X6,fitted=FALSE)
```


En las gráficas crPlots, se muestra que es posible ajustar tal vez un polinomio para X4 y X6. (Chunk crPlotsmodelo1, linea de código 109)  

```{r crPlotsmodelo1, echo=FALSE, include=FALSE}
library(car)
#Gráficas condicionales. 
#Dado el resto de variables, la relación es lineal?
#se compara con un polinomio (rosa) para ver si podría realizarse alguna modificación

crPlots(modelo1, order=2) #mucho problema con X4 y X6, jugar con order
#crPlots(modelo, terms= ~ X4, order=2)
```

```{r powerTransform1, echo=FALSE, include=FALSE}
summary(powerTransform(modelo1)) 
```
Haciendo la prueba ``powerTransform`` se rechaza la hipótesis nula de $\lambda =1$ por lo que hay que hacer una transformación a la variable dependiente $Y$, en la misma prueba se rechaza la hipótesis nula de la transformación logarítimica $\lambda =0$, y se presenta un valor sugerido de $2$ como exponente. (Chunk powerTransform1, linea de código 119). 


```{r boxTidwell1, echo=FALSE, include=FALSE}
boxTidwell(I((Y)^2)~I(X6+5.5), ~X1+X2+X3+X4+X5 , data=datos6)
boxTidwell(I((Y)^2)~ X4, ~X1+X2+X3+X5+X6 , data=datos6)
```

Luego, al hacer la prueba de ``boxTidwell`` para ver si se requiere transformar X4 y X6, tenemos que para el caso de X6 no se rechaza la hipótesis nula de una $\lambda = 1.032$, por lo que no requiere transformación, sin embargo, para el caso de X4, se muestra una $\lambda=0.38193$ que redondearemos a $1/2$, por lo que tomaremos la raíz cuadrada de la variable X4 en el modelado. (Chunk boxTidwell1, linea de codigo 125). 


Entonces, planteamos un segundo modelo que considera $Y^2$ en función de las covariables $X1$ a $X6$ considerando $\sqrt{X4}$ y los demás covariables sin cambios.  Los resultados se muestran en la segunda columna del Cuadro de ``MODELOS``, en donde  se observa la prueba global $F$ asociado a la tabla ANOVA cuyo p-value es menor a $0.05$, por lo que se rechaza la hipótesis nula $H_0: \hat{\beta_i}=0, \forall i=1,2,...,p$, a favor de la alternativa de que al menos un coeficiente estimado $\hat{\beta_i}$ es distinto de cero.  Se observa además que dado que están las otras variables, la variable X3 no agrega información adicional al modelado, lo mismo para el caso de X5. (Chunk modelo2, linea de código 137).  


```{r modelo2, echo=FALSE, include=FALSE}
#se considera un modelo con un polinomio de grado 2
#para la variable X6
modelo2=lm(I((Y)^2)~X1+X2+X3+I(X4^(1/2))+X5+X6, data=datos6)
summary(modelo2)
```


```{r residualplotsmodelo2, echo=FALSE, fig.height=4, fig.width=7 , include=FALSE}
#par(mfrow=c(1,1))
library(car)
residualPlots_modelo2<-residualPlots(modelo2) #se observan problemas con X4 y X6
```


Con este segundo modelo se cumple linealidad con la prueba Tukey, al igual que para todas las variables individuales. (Chunk residualplotsmodelo2, linea de código 144). 


```{r, echo=FALSE}
residualPlots_modelo2[,2]
```

```{r pruebasmodelo2, echo=FALSE, include=FALSE}
#Varianza constante 
#Se basa en los errores estandarizados o estudentizados
#Mismas pruebas usadas en regresión lineal simple:
library(lmtest)
sbpt2<-lmtest::bptest(modelo2) #NO se rechaza H0 de homocedasticidad, NO hay problemas
sbpt2p<-sbpt2$p.value #p-value de la prueba Breusch Pagan estudentizado

#Normalidad 
#Se basa en los residuales estandarizados o estudentizados
#Mismas pruebas que se usaron en regresi?n lineal simple:
library(broom)
Datosmodelo2=augment(modelo2)
swt2<-shapiro.test(Datosmodelo2$.std.resid) #Se rechaza H0 de normalidad, hay problemas
swt2p<-swt2$p.value

library(nortest)
kst2<-nortest::lillie.test(Datosmodelo2$.std.resid)#Se rechaza H0 de normalidad, hay problemas
kst2p<-kst2$p.value

library(tseries)
jbt2<-tseries::jarque.bera.test(Datosmodelo2$.std.resid)#Se rechaza H0 de normalidad, hay problemas
jbt2p<-jbt2$p.value
```

Además, de acuerdo con la prueba ``studentized Breusch-Pagan`` se tiene un p-value de `r sbpt2p` por lo que no se rechaza la hipótesis nula de homocesaticidad, mientras que las pruebas de normalidad Jarque-Bera, Shapiro-Wilk y Kolmogorov-Smirnov no rechazan la hipótesis nula de normalidad, con p-value de `r jbt2p`, `r swt2p` y `r kst2p`, respectivamente.  (Chunk pruebasmodelo2, línea de código 158).  

A partir de este segundo modelo, haremos una selección de variables. 

```{r subconjuntosleaps, echo=FALSE, include=FALSE}
#Mejor subconjunto
library(leaps)
subconjuntos<-regsubsets(I((Y)^2)~X1+X2+X3+I(X4^(1/2))+X5+X6, data=datos6, nbest=2, nvmax=10)
#summary(subconjuntos)

###algunas graficas interesantes para analizar los resultados con cada criterio
###bic, adjr2, Cp
plot(subconjuntos ,scale="bic")

####modelo descrito en renglón 15
coef(subconjuntos, 15)

modelo3=lm(I((Y)^2)~X1+X2+I(X4^(1/2))+X6, data=datos6) #modelo descrito en renglon 15
summary(modelo3)
("BIC")
BIC_modelo3<-BIC(modelo3)
BIC_modelo3
```

```{r stepBIC, echo=FALSE, include=FALSE}
#Metodos por pasos backward
##k es la penalizacion, 2 para AIC, log(n) para BIC
fitCompleto=lm(I((Y)^2)~X1+X2+X3+I(X4^(1/2))+X5+X6, data=datos6)
#step(fitCompleto,direction="backward", k = 2)   #Usando AIC y empezando con fitCompleto
#agregar trace=0 para eliminar la impresion de los pasos
step(fitCompleto,direction="backward", k = log(400), trace=0)  

#Metodos por pasos forward
##k es la penalizacion, 2 para AIC, log(n) para BIC
fitNulo=lm(I((Y)^2)~1, data=datos6)
step(fitNulo,direction="forward", k = log(400), scope=  ~ X1+X2+X3+I(X4^(1/2))+X5+X6, trace=0)  #Usando AIC y empezando con fitNulo
```

```{r glm_lambdabic, echo=FALSE, include=FALSE}
#Usando lasso #seleccion via penalizacion en la logverosimilitud
# paquete smurf #otra opcion mas popular es glmnet
library(smurf)
formu <- I((Y)^2) ~ p(X1, pen = "gflasso") + p(X2, pen = "gflasso")+
  p(X3, pen = "lasso") + p(I(X4^(1/2)), pen = "lasso") + p(X5, pen = "lasso") + p(X6, pen = "lasso")
#Usando la muestra completa
glm_lambdabic <- glmsmurf(formula = formu, family = gaussian(), data = datos6, 
                       pen.weights = "glm.stand", lambda = "is.bic")
plot_lambda(glm_lambdabic)
("lambda")
glm_lambdabic$lambda
BIC_glm<-BIC(glm_lambdabic)
plot(glm_lambdabic, cex=3)
summary(glm_lambdabic)
```

```{r modsellasso, echo=FALSE, include=FALSE}
glm_lambdabica <- glmsmurf(formula = formu, family = gaussian(), data = datos6, 
                         pen.weights = "glm.stand", lambda = glm_lambdabic$lambda, x.return=TRUE)
modsellasso=lm(V1~-1+.,as.data.frame(cbind(glm_lambdabica$y, as.matrix(glm_lambdabica$X.reest))) )
summary(modsellasso)
("BIC")
BIC_lasso<-BIC(modsellasso)
BIC_lasso
```

```{r, echo=FALSE, include=FALSE}
names(modsellasso$coefficients)<-c("(Intercept)", "X1A2", "X1A3.4", "X2B2.3", "X2B4", "I(X4^(1/2))", "X6")
modsellasso$coefficients
```



Con la función ``regsubsets`` de la biblioteca ``leaps`` se analizaron los mejores subconjuntos, se presenta la siguiente gráfica izquierda mostrando los resultados, con esto decidimos tomar las covariables X1, X2, X4 y X6, omitiendo las variables X3 y X5, con lo que se podría tener un tercer modelo que muestra un BIC de `r BIC_modelo3` y que se muestra en la columna 3 del Cuadro de MODELOS. (Chunk subconjuntosleaps, linea de código 187). Este resultado se refuerza con el análisis hecho con los métodos por pasos forward y backward, con la función ``step`` y el criterio BIC. (Chunk stepBIC, linea de código 207).  Adicionalmente se realizó con la biblioteca ``smurf`` un  modelo lasso de seleccion via penalizacion en la logverosimilitud,  considerando la familia gausiana, pesos glm.stand, y lambda is.bic, cuyo BIC es de `r BIC_glm` y resultado se muestra se presenta en la gráfica de la derecha (Chunk glm_lambdabic, linea de código 221). Como se puede observar en la gráfica derecha, las variables X3 y X5 tienen un valor de cero, además podemos notar que X1A3 y X1A4 pueden combinarse en una sola variable, al igual que  X2B2 y X2B3 en otra, así se realizó el ajuste con el modelo lasso final cuyos resultados se presentan en la cuarta y última columna del Cuadro de MODELOS y cuyo BIC es de `r BIC_lasso`. (Chunk modsellasso, linea de código 238). 


```{r, echo=FALSE, fig.width=9}
par(mfrow=c(1,2))
plot(subconjuntos ,scale="bic")
plot(glm_lambdabic, cex=3)
```



Finalmente, se presentan los resultados de los modelos referidos anteriormente en el Cuadro de MODELOS. El modelo final elegido es el de la última columna donde la variable dependiente es $V1=I((Y)ˆ2)$, podemos observar que la prueba $F$ tiene un p-value menor a $0.05$, los tres asteriscos indican que incluso menor que $0.01$, por lo que al menos un coeficiente estimado es distinto de cero. Además, cada uno de los coeficientes, en una análisis individual, dado que están las otras variables, agregan información al modelo.    

```{r, echo=FALSE}
#stargazer(modelo1, modelo2, modelo3, modsellasso, no.space=TRUE)
```

\begin{landscape}

\begin{table}[!htbp] \centering 
  \caption{MODELOS} 
  \label{} 
\footnotesize
\begin{tabular}{@{\extracolsep{5pt}}lcccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & Y & \multicolumn{2}{c}{I((Y)$\hat{\mkern6mu}$2)} & V1 \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4)\\ 
\hline \\[-1.8ex] 
 X3 & 0.002 & 0.036 &  &  \\ 
  & (0.003) & (0.028) &  &  \\ 
  X4 & 0.066$^{***}$ &  &  &  \\ 
  & (0.003) &  &  &  \\ 
  I(X4$\hat{\mkern6mu}$(1/2)) &  & 5.014$^{***}$ & 5.018$^{***}$ & 5.006$^{***}$ \\ 
  &  & (0.167) & (0.167) & (0.167) \\ 
  X5 & $-$0.002 & $-$0.016 &  &  \\ 
  & (0.002) & (0.018) &  &  \\ 
  X6 & $-$0.238$^{***}$ & $-$2.999$^{***}$ & $-$3.005$^{***}$ & $-$3.002$^{***}$ \\ 
  & (0.003) & (0.028) & (0.028) & (0.028) \\ 
  X1A2 & 1.295$^{***}$ & 16.979$^{***}$ & 16.967$^{***}$ & 16.969$^{***}$ \\ 
  & (0.014) & (0.152) & (0.151) & (0.151) \\ 
  X1A3 & 0.177$^{***}$ & 2.032$^{***}$ & 2.030$^{***}$ &  \\ 
  & (0.014) & (0.151) & (0.151) &  \\ 
  X1A4 & 0.177$^{***}$ & 2.019$^{***}$ & 2.014$^{***}$ &  \\ 
  & (0.014) & (0.152) & (0.151) &  \\ 
  X2B2 & $-$0.410$^{***}$ & $-$5.078$^{***}$ & $-$5.073$^{***}$ &  \\ 
  & (0.014) & (0.151) & (0.151) &  \\ 
  X2B3 & $-$0.410$^{***}$ & $-$5.248$^{***}$ & $-$5.244$^{***}$ &  \\ 
  & (0.014) & (0.151) & (0.151) &  \\ 
  X1A3.4 &  &  &  & 2.023$^{***}$ \\ 
  &  &  &  & (0.131) \\ 
  X2B2.3 &  &  &  & $-$5.159$^{***}$ \\ 
  &  &  &  & (0.131) \\ 
  X2B4 & $-$0.235$^{***}$ & $-$3.035$^{***}$ & $-$3.047$^{***}$ & $-$3.048$^{***}$ \\ 
  & (0.014) & (0.152) & (0.152) & (0.151) \\ 
  Constant & 5.550$^{***}$ & 23.262$^{***}$ & 23.410$^{***}$ & 23.448$^{***}$ \\ 
  & (0.030) & (0.548) & (0.541) & (0.540) \\ 
 \hline \\[-1.8ex] 
Observations & 400 & 400 & 400 & 400 \\ 
R$^{2}$ & 0.982 & 0.988 & 0.988 & 0.999 \\ 
Adjusted R$^{2}$ & 0.981 & 0.988 & 0.988 & 0.999 \\ 
Residual Std. Error & 0.102 (df = 389) & 1.065 (df = 389) & 1.066 (df = 391) & 1.065 (df = 393) \\ 
F Statistic & 2,101.510$^{***}$ (df = 10; 389) & 3,204.967$^{***}$ (df = 10; 389) & 3,999.207$^{***}$ (df = 8; 391) & 89,411.300$^{***}$ (df = 7; 393) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{4}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

\end{landscape}









