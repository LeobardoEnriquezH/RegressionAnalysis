---
title: ""
author: ""
date: ""
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, include=FALSE}
library(kableExtra)
library(tidyverse)
library(stargazer)
library(performance)
library(flextable)
library(see)
library(lmtest)
library(qqplotr)
library(ggrepel)
library(patchwork)
library(boot)
library(rempsyc)
library(report)
library(multcomp)
```


## 6. Uso del modelo de regresión lineal simple. 

A continuación se presentan los datos de los pesos de los huevos de 11 nidadas de pingüinos Macaroni, cada nidada tiene dos huevos, uno más pequeño (x) que el otro (y). 

```{r, echo=FALSE}
x=c(79, 93, 100, 105, 101, 96, 96, 109, 70, 71, 87)
y=c(123, 138, 154, 161, 155, 149, 152, 160, 117, 123, 138 )
Datos6=data.frame(cbind(x,y))
kable(t(Datos6)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
```

### I. Ajuste del modelo de regresión.

Ajustaremos una recta de regresión para estimar el peso promedio del huevo mayor (y) dado el peso del huevo menor (x), es decir, la variable dependiente es el peso del huevo más grande $y_i$ y la variable independiente es el peso del huevo menor. 


$$y_i=\beta_0+\beta_1x_i+\varepsilon_i$$ 



```{r, echo=FALSE, include=FALSE}
#Ajustamos nuestro modelo 
fit<-lm(y~x, data = Datos6)

#Y mostramos el summary con stargazer
stargazer(fit)
```


```{r, echo=FALSE, include=FALSE}
b0<-fit$coefficients[1]
b1<-fit$coefficients[2]
```


En el siguiente Cuadro podemos observar que el p-valor asociado a la prueba $F$ es de menor a 0.05, por lo que se rechaza la hipótesis nula de que los coeficientes asociados a las variables explicativas son cero contra la alternativa de que al menos un coeficiente estimado es distinto de cero. En este caso, como hay una solo variable explicativa, esta prueba coincide con la prueba $t-student$ individual para la $\beta_1$= `r b1`, que también rechaza la hipótesis nula de que $\beta_1=0$ contra la alternativa de que $\beta_1 \neq0$. 





\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\footnotesize
\begin{tabular}{@{\extracolsep{5pt}}lc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & y \\ 
\hline \\[-1.8ex] 
 x & 1.169$^{***}$ \\ 
  & s.e.(0.088) \\
  & t-value: 13.225 \\
  & Pr(>|t|): 3.35e-07\\
  & \\ 
 Constant & 35.674$^{***}$ \\ 
  & s.e.(8.171) \\
  & t-value: 4.366 \\
  & Pr(>|t|): 0.00181\\
  & \\ 
\hline \\[-1.8ex] 
Observations & 11 \\ 
R$^{2}$ & 0.951 \\ 
Adjusted R$^{2}$ & 0.946 \\ 
Residual Std. Error & 3.702 (df = 9) \\ 
F Statistic & 174.895$^{***}$ (df = 1; 9); p-value: 3.351e-07 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 


La Gráfica siguiente muestra las observaciones y la recta ajustada, parece ser que hay un muy buen ajuste de la recta a los puntos. 



```{r, echo=FALSE, fig.width = 6, fig.height = 2.8}
ggplot(Datos6, aes(x, y)) +
  geom_point() +
  geom_abline(intercept = b0, slope = b1) + 
  theme_bw()
```

En las siguientes Gráficas podemos observar  las pruebas gráficas para el cumplimiento de los supuestos del modelo de regresión lineal. La Gráfica **Residuals vs Fitted**, se utiliza para comprobar los supuestos de relación lineal, una línea horizontal, sin patrones distintos, es indicación de una relación lineal, lo que es bueno en nuestro caso. La Gráfica **Normal Q-Q**, se utiliza para examinar si los residuos se distribuyen normalmente, es bueno que los puntos residuales sigan la línea recta discontinua, en nuestro caso parece que todo se ajusta bien. La Gráfica **Scale-Location**, se utiliza para comprobar la homogeneidad de la varianza de los residuos (homoscedasticidad), la línea horizontal con puntos igualmente distribuidos es una buena indicación de homocedasticidad, este es el caso en nuestro ejemplo, donde no tenemos un problema de heterocedasticidad. La Gráfica **Residuals vs Leverage**, se utiliza para identificar casos de valores influyentes, es decir, valores extremos que podrían influir en los resultados de la regresión cuando se incluyen o excluyen del análisis, al parecer ningún valor sale de la distancia de Cook. 



```{r, echo=FALSE, fig.width = 6, fig.height = 4}
par(mfrow = c(2, 2))
plot(fit)
```



```{r, echo=FALSE, warning=FALSE,  message=FALSE, fig.width = 3, fig.height = 2.8, include=FALSE}
par(mfrow = c(2, 2))
#####check_model() function of performance package: Graphs####

# return a list of single plots
diagnostic_plots <- plot(check_model(fit, panel = FALSE))
# linearity
diagnostic_plots[[2]]
# normally distributed residuals
diagnostic_plots[[5]] #6
# homoscedasticiy - homogeneity of variance
diagnostic_plots[[3]]
# influential observations - outliers
diagnostic_plots[[4]]


```



En el siguiente Cuadro se pueden observar las pruebas de Shapiro-Wilk, Breusch-Pagan y Durbin-Watson, en todos los casos el p-value asociado es mayor a 0.05, por lo que no hay evidencia para rechzar las hipótesis nulas de normalidad, homoscedasticidad y no autocorrelación, respectivamente. 



```{r, echo=FALSE, warning=FALSE, message=FALSE}
#nice_assumptions from package rempsyc

table_tests<-nice_assumptions(fit)
table_tests_fin<-subset(table_tests, select = -c(Model,Diagnostic) )
```



```{r, echo=FALSE}
#nice_assumptions(fit)
#table_nice
kable(t(table_tests_fin)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)

```




### II. Prueba de hipótesis. Diferencia entre peso mayor y menor como constante. 

Ante la sospecha de que en promedio la diferencia entre el peso mayor y el peso menor es constante (es decir, no depende del peso del huevo menor observado), haremos una prueba de hipótesis. En primer lugar notemos que esto implicaría $H_0: \beta_1=1$, contra la alternativa $\beta_1\neq1$, a continuación se muestra la aprueba ``Simultaneous Tests for General Linear Hypotheses``, donde no se rechaza esta hipótesis nula, pues el p-valor asociado es mayor a 0.05, considerando un nivel del confianza del 95%.  


```{r, echo=FALSE}
# Pruebas de hipótesis para beta1
Matriz=matrix(c(0,1), ncol=2, nrow=1)
c=1
#alternative: "two.sided" (default), "greater" or "less"
prueba1=glht(fit, linfct=Matriz, rhs=c, alternative="two.sided")
#summary(prueba1)
```


\begin{table}[!htbp] \centering
\footnotesize
\begin{tabular}{|lllll|}
\hline
\multicolumn{5}{|c|}{Simultaneous Tests for General Linear Hypotheses}                                                                                                \\ 
\multicolumn{5}{|c|}{Fit: lm(formula = Y $\sim$x, data = Datos6)}                                                                                                    \\
\multicolumn{5}{|l|}{Linear Hypotheses:}                                                                                                                            \\ \hline
\multicolumn{1}{|l|}{}                    & \multicolumn{1}{l|}{Estimate} & \multicolumn{1}{l|}{Std. Error} & \multicolumn{1}{l|}{t value}         & Pr(\textless{}t) \\ \hline
\multicolumn{1}{|l|}{1 == 1} & \multicolumn{1}{l|}{1.16940}  & \multicolumn{1}{l|}{0.08842}     & \multicolumn{1}{l|}{1.916}         & 0.0877           \\ \hline
\multicolumn{5}{|l|}{}                             \\ \hline
\multicolumn{1}{|l|}{Signif. codes:}      & \multicolumn{4}{l|}{0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1}                                                       \\ \hline
\multicolumn{5}{|l|}{(Adjusted p values reported -- single-step method)}                                                                                              \\ \hline
\end{tabular}
\end{table}



### III. Nueva observación (nidada). ¿Los huevos provienes de pinguinos Macaroni?


Se observa el peso de los huevos de una nueva nidada, observándose un peso de 70 y 145 gramos. Usando un intervalo de confianza del 95%, veremos si la nidada de huevos sí proviene de pinguinos Macaroni. 



```{r, echo=FALSE}
#cálculo del valor para un peso de 70
nuevo_y<-b0+b1*70
```

Si tomamos en cuenta la recta de regresión anterior, podemos ver que $y_i=\hat{\beta_0}+\hat{\beta_1}x_i$ = `r b0` + `r b1` *(70)= `r nuevo_y`. Éste valor se ve alejado de los 145 gramos del huevo más grande encontrado.





```{r, echo=FALSE}
new.dat <- data.frame(x = c(70))
predict<-predict(fit, newdata = new.dat, interval = 'prediction',  level = 0.95)
#predict[3]
```



Podemos predecir que el valor del huevo más grande debería estar entre el valor `r predict[2]` y el valor `r predict[3]` de acuerdo con un intervalo de predicción. El valor de 145 gramos no cae dentro del intervalo, como también podemos observar en la siguiente Gráfica, por lo que podríamos concluir que la nueva observación no corresponde a los huevos de los pingüinos Macaroni. 



```{r, echo=FALSE}
# Extract the prediction interval values
lower_limit <- predict[1, "lwr"]
predicted_value <- predict[1, "fit"]
upper_limit <- predict[1, "upr"]

# Print the prediction interval values
#cat("Lower Limit:", lower_limit, "\n")
#cat("Predicted Sales:", predicted_sales, "\n")
#cat("Upper Limit:", upper_limit, "\n")
```


```{r, echo=FALSE, fig.width = 6, fig.height = 4}

# Plot the prediction interval on a graph
plot(Datos6$x, Datos6$y, main = "", xlab = "X", ylab = "Y", pch = 16)
# Add the regression line
abline(fit, col = "green")  
# Add the predicted point
points(70, predicted_value, col = "red", pch = 16)
points(70, 145, col = "blue", pch = 16)
segments(70, lower_limit, 70, upper_limit, col = "red", lwd = 2)
```
















