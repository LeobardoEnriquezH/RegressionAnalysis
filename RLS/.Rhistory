#nice_assumptions from package rempsyc
table_tests<-nice_assumptions(modelo2)
table_tests_fin<-subset(table_tests, select = -c(Model,Diagnostic) )
table_tests_fin<-table_tests_fin[,1:2]
#nice_assumptions()
#table_nice
kable(t(table_tests_fin)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
#Normalidad: Ho:Hay normalidad.
library(nortest)
datos_m2<-augment(modelo2)
normalidad_m2<-ad.test(datos_m2$.std.resid)
#No se rechaza Ho, p-value >0.05
#Homocedasticidad: Ho:Varianza constantes.
homocedasticidad_m2<-car::ncvTest(modelo2)
#Se rechaza Ho, p-value <0.05
#Linealidad
library(car)
residualPlots(modelo2)
#Ho: no se necesita transformación para linealidad
#Se rechaza Ho, p-value <0.05
modelo2<-lm(data=datos7, I(api00^2) ~ meals)
#summary(modelo2)
stargazer(modelo2)
modelo2<-lm(data=datos7, I(api00^2) ~ meals)
summary(modelo2)
#stargazer(modelo2)
#Linealidad
library(car)
residualPlots(modelo1)
#Ho: no se necesita transformación para linealidad
#Se rechaza Ho, p-value <0.05
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(tidyverse)
library(stargazer)
library(performance)
library(flextable)
library(see)
library(lmtest)
library(qqplotr)
library(ggrepel)
library(patchwork)
library(boot)
library(rempsyc)
library(report)
library(multcomp)
library(car)
library(broom)
datos7<-read_csv("performance.csv", show_col_types = FALSE)
modelo1<-lm(data=datos7, api00~meals)
#summary(modelo1)
#stargazer(modelo1)
#nice_assumptions from package rempsyc
table_tests<-nice_assumptions(modelo1)
table_tests_fin<-subset(table_tests, select = -c(Model,Diagnostic) )
table_tests_fin<-table_tests_fin[,1:2]
#nice_assumptions()
#table_nice
kable(t(table_tests_fin)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
#Normalidad: Ho:Hay normalidad.
library(nortest)
datos_m1<-augment(modelo1)
normalidad_m1<-ad.test(datos_m1$.std.resid)
#No se rechaza Ho, p-value >0.05
#Homocedasticidad: Ho:Varianza constantes.
library(car)
homocedasticidad_m1<-car::ncvTest(modelo1)
#Se rechaza Ho, p-value <0.05
#Linealidad
library(car)
residualPlots(modelo1)
#Ho: no se necesita transformación para linealidad
#Se rechaza Ho, p-value <0.05
par(mfrow = c(2, 2))
plot(modelo1)
par(mfrow = c(2, 2))
#####check_model() function of performance package: Graphs####
# return a list of single plots
diagnostic_plots <- plot(check_model(modelo1, panel = FALSE))
# linearity
diagnostic_plots[[2]]
# normally distributed residuals
diagnostic_plots[[5]] #6
# homoscedasticiy - homogeneity of variance
diagnostic_plots[[3]]
# influential observations - outliers
diagnostic_plots[[4]]
bc<-car::powerTransform(modelo1)
cat("PowerTransform: \n")
bc
datos7$meals2<-datos7$meals+1 #sumamos 1 porque hay un valor de 0 (no positivo)
cat("BoxTidwell: \n")
car::boxTidwell(api00^2~ I(meals+1), data = datos7)
modelo2<-lm(data=datos7, I(api00^2) ~ meals)
#summary(modelo2)
#stargazer(modelo2)
#nice_assumptions from package rempsyc
table_tests<-nice_assumptions(modelo2)
table_tests_fin<-subset(table_tests, select = -c(Model,Diagnostic) )
table_tests_fin<-table_tests_fin[,1:2]
#nice_assumptions()
#table_nice
kable(t(table_tests_fin)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
#Normalidad: Ho:Hay normalidad.
library(nortest)
datos_m2<-augment(modelo2)
normalidad_m2<-ad.test(datos_m2$.std.resid)
#No se rechaza Ho, p-value >0.05
#Homocedasticidad: Ho:Varianza constantes.
homocedasticidad_m2<-car::ncvTest(modelo2)
#Se rechaza Ho, p-value <0.05
#Linealidad
library(car)
residualPlots(modelo2)
#Ho: no se necesita transformación para linealidad
#Se rechaza Ho, p-value <0.05
par(mfrow = c(2, 2))
plot(modelo2)
par(mfrow = c(2, 2))
#####check_model() function of performance package: Graphs####
# return a list of single plots
diagnostic_plots <- plot(check_model(modelo2, panel = FALSE))
# linearity
diagnostic_plots[[2]]
# normally distributed residuals
diagnostic_plots[[5]] #6
# homoscedasticiy - homogeneity of variance
diagnostic_plots[[3]]
# influential observations - outliers
diagnostic_plots[[4]]
curva_ajustada1 <- function(x) {modelo1$coefficients[1] + modelo1$coefficients[2]*x}
curva_ajustada2 <- function(x) {sqrt(modelo2$coefficients[1] + modelo2$coefficients[2]*x)}
ggplot(datos7, aes(meals, api00)) +
geom_point() +
geom_function(fun = curva_ajustada1, col="red") +
geom_function(fun = curva_ajustada2, col="blue") + theme_bw()
# Pruebas de hipótesis para beta1
Matriz=matrix(c(0,1), ncol=2, nrow=1)
c=0
#alternative: "two.sided" (default), "greater" or "less"
prueba1=glht(modelo2, linfct=Matriz, rhs=c, alternative="less")
summary(prueba1)
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(tidyverse)
library(stargazer)
library(performance)
library(flextable)
library(see)
library(lmtest)
library(qqplotr)
library(ggrepel)
library(patchwork)
library(boot)
library(rempsyc)
library(report)
library(multcomp)
library(car)
library(broom)
datos7<-read_csv("performance.csv", show_col_types = FALSE)
modelo1<-lm(data=datos7, api00~meals)
#summary(modelo1)
#stargazer(modelo1)
#nice_assumptions from package rempsyc
table_tests<-nice_assumptions(modelo1)
table_tests_fin<-subset(table_tests, select = -c(Model,Diagnostic) )
table_tests_fin<-table_tests_fin[,1:2]
#nice_assumptions()
#table_nice
kable(t(table_tests_fin)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
#Normalidad: Ho:Hay normalidad.
library(nortest)
datos_m1<-augment(modelo1)
normalidad_m1<-ad.test(datos_m1$.std.resid)
#No se rechaza Ho, p-value >0.05
#Homocedasticidad: Ho:Varianza constantes.
library(car)
homocedasticidad_m1<-car::ncvTest(modelo1)
#Se rechaza Ho, p-value <0.05
#Linealidad
library(car)
residualPlots(modelo1)
#Ho: no se necesita transformación para linealidad
#Se rechaza Ho, p-value <0.05
par(mfrow = c(2, 2))
plot(modelo1)
par(mfrow = c(2, 2))
#####check_model() function of performance package: Graphs####
# return a list of single plots
diagnostic_plots <- plot(check_model(modelo1, panel = FALSE))
# linearity
diagnostic_plots[[2]]
# normally distributed residuals
diagnostic_plots[[5]] #6
# homoscedasticiy - homogeneity of variance
diagnostic_plots[[3]]
# influential observations - outliers
diagnostic_plots[[4]]
bc<-car::powerTransform(modelo1)
cat("PowerTransform: \n")
bc
datos7$meals2<-datos7$meals+1 #sumamos 1 porque hay un valor de 0 (no positivo)
cat("BoxTidwell: \n")
car::boxTidwell(api00^2~ I(meals+1), data = datos7)
modelo2<-lm(data=datos7, I(api00^2) ~ meals)
#summary(modelo2)
#stargazer(modelo2)
#nice_assumptions from package rempsyc
table_tests<-nice_assumptions(modelo2)
table_tests_fin<-subset(table_tests, select = -c(Model,Diagnostic) )
table_tests_fin<-table_tests_fin[,1:2]
#nice_assumptions()
#table_nice
kable(t(table_tests_fin)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
#Normalidad: Ho:Hay normalidad.
library(nortest)
datos_m2<-augment(modelo2)
normalidad_m2<-ad.test(datos_m2$.std.resid)
#No se rechaza Ho, p-value >0.05
#Homocedasticidad: Ho:Varianza constantes.
homocedasticidad_m2<-car::ncvTest(modelo2)
#Se rechaza Ho, p-value <0.05
#Linealidad
library(car)
residualPlots(modelo2)
#Ho: no se necesita transformación para linealidad
#Se rechaza Ho, p-value <0.05
par(mfrow = c(2, 2))
plot(modelo2)
par(mfrow = c(2, 2))
#####check_model() function of performance package: Graphs####
# return a list of single plots
diagnostic_plots <- plot(check_model(modelo2, panel = FALSE))
# linearity
diagnostic_plots[[2]]
# normally distributed residuals
diagnostic_plots[[5]] #6
# homoscedasticiy - homogeneity of variance
diagnostic_plots[[3]]
# influential observations - outliers
diagnostic_plots[[4]]
curva_ajustada1 <- function(x) {modelo1$coefficients[1] + modelo1$coefficients[2]*x}
curva_ajustada2 <- function(x) {sqrt(modelo2$coefficients[1] + modelo2$coefficients[2]*x)}
ggplot(datos7, aes(meals, api00)) +
geom_point() +
geom_function(fun = curva_ajustada1, col="red") +
geom_function(fun = curva_ajustada2, col="blue") + theme_bw()
# Pruebas de hipótesis para beta1
Matriz=matrix(c(0,1), ncol=2, nrow=1)
c=0
#alternative: "two.sided" (default), "greater" or "less"
prueba1=glht(modelo2, linfct=Matriz, rhs=c, alternative="less")
summary(prueba1)
bc<-car::powerTransform(modelo1)
#cat("PowerTransform: \n")
bc
datos7$meals2<-datos7$meals+1 #sumamos 1 porque hay un valor de 0 (no positivo)
#cat("BoxTidwell: \n")
car::boxTidwell(api00^2~ I(meals+1), data = datos7)
knitr::opts_chunk$set(echo = T, fig.width = 6, fig.height = 3.5)
rm(list = ls())
pacman::p_load(tidyverse,
kableExtra,
cowplot,
stargazer,knitr,viridis,dplyr,readr,scales,quantmod,texreg,tinytex,
tidyr, imager,lubridate,tseries, astsa, growthrates, tis, dynlm,
readxl, foreign, hrbthemes, gtsummary, corrplot, lm.beta, ggfortify,
AER, lmtest, sandwich,GGally, performance, flextable, see, qqplotr,
ggrepel, patchwork,boot, rempsyc, report,multcomp, car, broom)
#library(tidyverse) #read_csv
datos<-read_csv("Ejercicio5B.csv", col_names = TRUE)
#Hacemos los datos no numericos como factor
datos$Med<-factor(datos$Med)
datos$Edad<-factor(datos$Edad)
min<-min(datos$Y)
max<-max(datos$Y)
sd<-sd(datos$Y)
mean<-mean(datos$Y)
cargaviral<-as.data.frame(datos$Y)
#stargazer(cargaviral)
# Definimos colores
custom_colors <- c("steelblue", "darkorange")
# Generamos el Boxplot
ggplot(datos, aes(x = Med, y = Y, fill = Med)) +
geom_boxplot(width = 0.3, alpha = 0.5, outlier.shape = 1, outlier.colour = "red") +
#geom_jitter(width = 0.2, height = 0, size = 1, alpha = 0.8) +
scale_fill_manual(values = custom_colors) +
labs(x = "Med", y = "Carga viral") +
ggtitle("Box Plot con Med (tratados y no tratados)") +
theme_minimal() +
theme(
plot.title = element_text(size = 12, face = "bold"),
axis.text = element_text(size = 8),
axis.title = element_text(size = 10, face = "bold"),
legend.title = element_blank(),
legend.position = "none")
#Realizamos un relevel para poner como referencia "No"
datos$Med<-relevel(datos$Med,"No")
#levels(datos$Med)
#Ajustamos nuestro modelo
fit1<-lm(Y~Med, data = datos)
#Y mostramos el summary con stargazer
#stargazer(fit1)
b0<-fit1$coefficients[1]
b1<-fit1$coefficients[2]
# Pruebas de hipótesis para beta1
Matriz=matrix(c(0,1), ncol=2, nrow=1)
c=0
#alternative: "two.sided" (default), "greater" or "less"
prueba1=glht(fit1, linfct=Matriz, rhs=c, alternative="less")
summary(prueba1)
table_tests<-nice_assumptions(fit1)
table_tests_fin<-subset(table_tests, select = -c(Model,Diagnostic) )
kable(t(table_tests_fin)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
# Crear interacción entre factores
datos$Med_Edad <- interaction(datos$Med, datos$Edad)
# Definimos colores
custom_colors <- c("steelblue", "darkorange", "forestgreen", "firebrick")
# Generamos el Boxplot
ggplot(datos, aes(x = Med_Edad, y = Y, fill = Med_Edad)) +
geom_boxplot(width = 0.3, alpha = 0.5, outlier.shape = 1, outlier.colour = "red") +
#geom_jitter(width = 0.2, height = 0, size = 1, alpha = 0.8) +
scale_fill_manual(values = custom_colors) +
labs(x = "Med & Edad", y = "Carga viral") +
ggtitle("Box Plot con Med & Edad") +
theme_minimal() +
theme(
plot.title = element_text(size = 12, face = "bold"),
axis.text = element_text(size = 8),
axis.title = element_text(size = 10, face = "bold"),
legend.title = element_blank(),
legend.position = "none")
#Ajustamos nuestro modelo
datos_cortados<-datos%>%filter(Edad==">60")
#Ajustamos nuestro modelo
fit2<-lm(Y~Med, data = datos_cortados)
#Y mostramos el summary con stargazer
stargazer(fit2)
b0<-fit2$coefficients[1]
b1<-fit2$coefficients[2]
fit3<-lm(Y~Med+Edad, data = datos)
#summary(fit3)
table_tests<-nice_assumptions(fit3)
table_tests_fin<-subset(table_tests, select = -c(Model,Diagnostic) )
kable(t(table_tests_fin)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
#stargazer(fit1,fit2,fit3)
x=c(79, 93, 100, 105, 101, 96, 96, 109, 70, 71, 87)
y=c(123, 138, 154, 161, 155, 149, 152, 160, 117, 123, 138 )
Datos6=data.frame(cbind(x,y))
kable(t(Datos6)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
#Ajustamos nuestro modelo
fit<-lm(y~x, data = Datos6)
#Y mostramos el summary con stargazer
stargazer(fit)
b0<-fit$coefficients[1]
b1<-fit$coefficients[2]
ggplot(Datos6, aes(x, y)) +
geom_point() +
geom_abline(intercept = b0, slope = b1) +
theme_bw()
par(mfrow = c(2, 2))
plot(fit)
par(mfrow = c(2, 2))
#####check_model() function of performance package: Graphs####
# return a list of single plots
diagnostic_plots <- plot(check_model(fit, panel = FALSE))
# linearity
diagnostic_plots[[2]]
# normally distributed residuals
diagnostic_plots[[5]] #6
# homoscedasticiy - homogeneity of variance
diagnostic_plots[[3]]
# influential observations - outliers
diagnostic_plots[[4]]
#nice_assumptions from package rempsyc
table_tests<-nice_assumptions(fit)
table_tests_fin<-subset(table_tests, select = -c(Model,Diagnostic) )
#nice_assumptions(fit)
#table_nice
kable(t(table_tests_fin)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
# Pruebas de hipótesis para beta1
Matriz=matrix(c(0,1), ncol=2, nrow=1)
c=1
#alternative: "two.sided" (default), "greater" or "less"
prueba1=glht(fit, linfct=Matriz, rhs=c, alternative="two.sided")
#summary(prueba1)
#cálculo del valor para un peso de 70
nuevo_y<-b0+b1*70
new.dat <- data.frame(x = c(70))
predict<-predict(fit, newdata = new.dat, interval = 'prediction',  level = 0.95)
#predict[3]
# Extract the prediction interval values
lower_limit <- predict[1, "lwr"]
predicted_value <- predict[1, "fit"]
upper_limit <- predict[1, "upr"]
# Print the prediction interval values
#cat("Lower Limit:", lower_limit, "\n")
#cat("Predicted Sales:", predicted_sales, "\n")
#cat("Upper Limit:", upper_limit, "\n")
# Plot the prediction interval on a graph
plot(Datos6$x, Datos6$y, main = "", xlab = "X", ylab = "Y", pch = 16)
# Add the regression line
abline(fit, col = "green")
# Add the predicted point
points(70, predicted_value, col = "red", pch = 16)
points(70, 145, col = "blue", pch = 16)
segments(70, lower_limit, 70, upper_limit, col = "red", lwd = 2)
datos7<-read_csv("performance.csv", show_col_types = FALSE)
modelo1<-lm(data=datos7, api00~meals)
#summary(modelo1)
#stargazer(modelo1)
#nice_assumptions from package rempsyc
table_tests<-nice_assumptions(modelo1)
table_tests_fin<-subset(table_tests, select = -c(Model,Diagnostic) )
table_tests_fin<-table_tests_fin[,1:2]
#nice_assumptions()
#table_nice
kable(t(table_tests_fin)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
#Normalidad: Ho:Hay normalidad.
library(nortest)
datos_m1<-augment(modelo1)
normalidad_m1<-ad.test(datos_m1$.std.resid)
#No se rechaza Ho, p-value >0.05
#Homocedasticidad: Ho:Varianza constantes.
library(car)
homocedasticidad_m1<-car::ncvTest(modelo1)
#Se rechaza Ho, p-value <0.05
#Linealidad
library(car)
residualPlots(modelo1)
#Ho: no se necesita transformación para linealidad
#Se rechaza Ho, p-value <0.05
par(mfrow = c(2, 2))
plot(modelo1)
par(mfrow = c(2, 2))
#####check_model() function of performance package: Graphs####
# return a list of single plots
diagnostic_plots <- plot(check_model(modelo1, panel = FALSE))
# linearity
diagnostic_plots[[2]]
# normally distributed residuals
diagnostic_plots[[5]] #6
# homoscedasticiy - homogeneity of variance
diagnostic_plots[[3]]
# influential observations - outliers
diagnostic_plots[[4]]
bc<-car::powerTransform(modelo1)
#cat("PowerTransform: \n")
bc
datos7$meals2<-datos7$meals+1 #sumamos 1 porque hay un valor de 0 (no positivo)
#cat("BoxTidwell: \n")
car::boxTidwell(api00^2~ I(meals+1), data = datos7)
modelo2<-lm(data=datos7, I(api00^2) ~ meals)
#summary(modelo2)
#stargazer(modelo2)
#nice_assumptions from package rempsyc
table_tests<-nice_assumptions(modelo2)
table_tests_fin<-subset(table_tests, select = -c(Model,Diagnostic) )
table_tests_fin<-table_tests_fin[,1:2]
#nice_assumptions()
#table_nice
kable(t(table_tests_fin)) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
#Normalidad: Ho:Hay normalidad.
library(nortest)
datos_m2<-augment(modelo2)
normalidad_m2<-ad.test(datos_m2$.std.resid)
#No se rechaza Ho, p-value >0.05
#Homocedasticidad: Ho:Varianza constantes.
homocedasticidad_m2<-car::ncvTest(modelo2)
#Se rechaza Ho, p-value <0.05
#Linealidad
library(car)
residualPlots(modelo2)
#Ho: no se necesita transformación para linealidad
#Se rechaza Ho, p-value <0.05
par(mfrow = c(2, 2))
plot(modelo2)
par(mfrow = c(2, 2))
#####check_model() function of performance package: Graphs####
# return a list of single plots
diagnostic_plots <- plot(check_model(modelo2, panel = FALSE))
# linearity
diagnostic_plots[[2]]
# normally distributed residuals
diagnostic_plots[[5]] #6
# homoscedasticiy - homogeneity of variance
diagnostic_plots[[3]]
# influential observations - outliers
diagnostic_plots[[4]]
curva_ajustada1 <- function(x) {modelo1$coefficients[1] + modelo1$coefficients[2]*x}
curva_ajustada2 <- function(x) {sqrt(modelo2$coefficients[1] + modelo2$coefficients[2]*x)}
ggplot(datos7, aes(meals, api00)) +
geom_point() +
geom_function(fun = curva_ajustada1, col="red") +
geom_function(fun = curva_ajustada2, col="blue") + theme_bw()
# Pruebas de hipótesis para beta1
Matriz=matrix(c(0,1), ncol=2, nrow=1)
c=0
#alternative: "two.sided" (default), "greater" or "less"
prueba1=glht(modelo2, linfct=Matriz, rhs=c, alternative="less")
summary(prueba1)
